{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling for Political Analysis\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Imports: Packages to be used by the Bayesian Modeling Process\n",
    "2. Constants: Variables that are used throughout the modeling process\n",
    "3. Fit Bayesian Models to the data\n",
    "    - \n",
    "    - \n",
    "    - \n",
    "    - \n",
    "    - \n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as db\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Bayesian Model Estimating One Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    theta = pm.Beta('theta', alpha = 0.1, beta = 0.1, shape = regions)\n",
    "    y = pm.Bernoulli('y', p = theta[idx], observed = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in DEMO_COLS:\n",
    "    df.loc[:,col] = df[col].apply(lambda x: x if x >= 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['REPUBLICAN_PCT','YR','RGN_DESC'] + DEMO_COLS + STD_COLS  + SCALE_COLS]# + NN_COLS]\n",
    "X.loc[:,'MED_HH_INC'] = (X['MED_HH_INC'] / 1000).astype(int)\n",
    "X.loc[:,'PER_CAP_INC'] = (X['PER_CAP_INC'] / 1000).astype(int)\n",
    "X.loc[:,'POP_EMPLOYED'] = X['POP_EMPLOYED'] / X['TOT_POP_CNTY'] * 100\n",
    "X.loc[:,'REPUBLICAN_PCT'] = X['REPUBLICAN_PCT'] * 100\n",
    "X.loc[:,'TOT_POP_CNTY'] = np.log(X['TOT_POP_CNTY'] / 1000) \n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(X['REPUBLICAN_PCT'], bins='auto')\n",
    "plt.title(\"Histograms of REPUBLICAN_PCT\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X16 = X[X['YR'] == 2016]\n",
    "X16.drop(columns = ['YR'], inplace = True)\n",
    "X12 = X[X['YR'] == 2012]\n",
    "\n",
    "regions = X16.RGN_DESC.unique()\n",
    "regions = pd.DataFrame(regions, columns=['REGION'])\n",
    "regions['i'] = regions.index\n",
    "\n",
    "X16 = pd.merge(X16, regions, left_on='RGN_DESC', right_on='REGION', how='left')\n",
    "X16 = X16.rename(columns = {'i': 'ix_region'}).drop('REGION', 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X16.iloc[:,2:], X16['REPUBLICAN_PCT'], test_size = 0.3, random_state=42)\n",
    "\n",
    "region = X_train.ix_region.values\n",
    "X_train.drop(columns = 'ix_region', inplace = True)\n",
    "#you are modelling the standard deviation of a normal distribution with a normal. The test point for that is 0.0, which has 0 probability of occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Pooling Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    intercept = pm.Normal('intercept', mu= 50, sigma=10, shape = len(regions['i']))\n",
    "    grp_sd = pm.Uniform('grp_sd', 0, 10)\n",
    "\n",
    "    y_hat = intercept[region] \n",
    "    for ix, var in enumerate(X_train):\n",
    "        y_hat += pm.Normal(\"beta_{}\".format(ix), mu= 0, sigma = grp_sd, shape=len(regions['i']))[region] * X_train[col].values\n",
    "\n",
    "    # Model error\n",
    "    sigma_y = pm.HalfCauchy('sigma_y', beta = 1)\n",
    "    \n",
    "    # Creating the model requires a formula and data (and optionally a family)\n",
    "    likelihood = pm.Normal('y', mu=y_hat, sd=sigma_y, observed=y_train.values)\n",
    "\n",
    "    # Perform Markov Chain Monte Carlo sampling letting PyMC3 choose the algorithm\n",
    "    trace = pm.sample(2000, step = pm.NUTS(), cores = 1)#, init = 'advi+adapt_diag_grad'\n",
    "\n",
    "    model_formula = 'REPUBLICAN_PCT = '\n",
    "    for variable in trace.varnames:\n",
    "        model_formula += ' %0.2f * %s +' % (np.mean(trace[variable]), variable)\n",
    "        \n",
    "    print(model_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.forestplot(trace, var_names=['intercept'])\n",
    "ax[0].set_yticklabels(regions['REGION'].values)\n",
    "ax[0].set_title('Regions Intercept');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.forestplot(trace, var_names=['beta_34'])\n",
    "ax[0].set_yticklabels(regions['REGION'].values)\n",
    "ax[0].set_title('Total Population by Region');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pm.traceplot(trace, var_names=['intercept', 'beta_33']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGN_TRACE = {}\n",
    "\n",
    "for rgn in X['RGN_DESC'].unique():\n",
    "    X_RGN = X[X['RGN_DESC'] == rgn]\n",
    "    df12 = X_RGN[X_RGN['YR'] == 2012]\n",
    "    df12.drop(columns = ['RGN_DESC','YR'], inplace = True)\n",
    "    \n",
    "    df16 = X_RGN[X_RGN['YR'] == 2016]\n",
    "    df16.drop(columns = ['RGN_DESC','YR'], inplace = True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df16, df16['REPUBLICAN_PCT'], test_size = 0.1, random_state=42)\n",
    "    \n",
    "    formula = \"REPUBLICAN_PCT ~ \" + \" + \".join(list(df12)[1:])\n",
    "    with pm.Model() as model:\n",
    "        #set variable means and sd\n",
    "        grp_mean = pm.Normal('grp_mean', mu= 50, sigma=10)\n",
    "        grp_sd = pm.Uniform('grp_sd', 0, 10)\n",
    "        \n",
    "        # The prior for the data likelihood is a Normal REPUBLICAN_PCT\n",
    "        priors = {'Intercept': pm.Normal.dist(mu=df12['REPUBLICAN_PCT'].mean(), \n",
    "                                            sigma=df12['REPUBLICAN_PCT'].std())}\n",
    "        for col in list(df12)[1:]:\n",
    "            priors[col] =  pm.Normal.dist(mu=grp_mean, sigma=grp_sd)\n",
    "\n",
    "        # Creating the model requires a formula and data (and optionally a family)\n",
    "        pm.GLM.from_formula(formula, data = X_train, family = pm.glm.families.Normal(), priors = priors)\n",
    "\n",
    "        # Perform Markov Chain Monte Carlo sampling letting PyMC3 choose the algorithm\n",
    "        trace = pm.sample(2000, step = pm.NUTS(), cores = 1)\n",
    "\n",
    "    RGN_TRACE[rgn] = trace\n",
    "    \n",
    "    evaluate_trace(trace, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a table to print out all of the coefficients together\n",
    "i = 0\n",
    "full_df = []\n",
    "for key, normal_trace in RGN_TRACE.items():\n",
    "    if i == 0:\n",
    "        column_names = ['Region'] + pm.summary(normal_trace).index.tolist()\n",
    "        i+=1\n",
    "    row = [key] + pm.summary(normal_trace)['mean'].round(2).values.tolist()\n",
    "    full_df.append(row)\n",
    "    \n",
    "df_coef = pd.DataFrame(full_df, columns = column_names)\n",
    "df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_PATH = PROJ + 'data/raw/demographics/'\n",
    "IND_PATH = PROJ + 'data/raw/indicators/'\n",
    "PROC_PATH = PROJ + 'data/processed/'\n",
    "SQL_US_ANALYSIS =  db.connect(PROC_PATH + 'us_pop_factors.db')\n",
    "DEMO_EX = [ 'IA_FEMALE_BOOMER', 'IA_FEMALE_OLD', 'IA_FEMALE_YOUNG', 'IA_FEMALE_YOUNG_PROF', 'IA_MALE_BOOMER', 'IA_MALE_OLD', \n",
    "              'IA_MALE_YOUNG', 'IA_MALE_YOUNG_PROF', 'NA_FEMALE_BOOMER', 'NA_FEMALE_OLD', 'NA_FEMALE_YOUNG', \n",
    "              'NA_FEMALE_YOUNG_PROF', 'NA_MALE_BOOMER', 'NA_MALE_OLD', 'NA_MALE_YOUNG', 'NA_MALE_YOUNG_PROF', \n",
    "              'TOM_FEMALE_BOOMER', 'TOM_FEMALE_OLD', 'TOM_FEMALE_YOUNG', 'TOM_FEMALE_YOUNG_PROF', \n",
    "              'TOM_MALE_BOOMER', 'TOM_MALE_OLD', 'TOM_MALE_YOUNG', 'TOM_MALE_YOUNG_PROF']\n",
    "DEMO_COLS = ['AA_FEMALE_BOOMER', 'AA_FEMALE_OLD', 'AA_FEMALE_YOUNG', 'AA_FEMALE_YOUNG_PROF', 'AA_MALE_BOOMER', \n",
    "              'AA_MALE_OLD', 'AA_MALE_YOUNG', 'AA_MALE_YOUNG_PROF', 'BA_FEMALE_BOOMER', 'BA_FEMALE_OLD', \n",
    "              'BA_FEMALE_YOUNG', 'BA_FEMALE_YOUNG_PROF', 'BA_MALE_BOOMER', 'BA_MALE_OLD', 'BA_MALE_YOUNG', \n",
    "              'BA_MALE_YOUNG_PROF', 'H_FEMALE_BOOMER', 'H_FEMALE_OLD', 'H_FEMALE_YOUNG', 'H_FEMALE_YOUNG_PROF', \n",
    "              'H_MALE_BOOMER', 'H_MALE_OLD', 'H_MALE_YOUNG', 'H_MALE_YOUNG_PROF',\n",
    "              'WA_FEMALE_BOOMER', 'WA_FEMALE_OLD', 'WA_FEMALE_YOUNG', 'WA_FEMALE_YOUNG_PROF', \n",
    "              'WA_MALE_BOOMER', 'WA_MALE_OLD', 'WA_MALE_YOUNG', 'WA_MALE_YOUNG_PROF']\n",
    "STD_COLS = ['MED_HH_INC', 'PER_CAP_INC','TOT_POP_CNTY']\n",
    "NN_COLS = ['URBAN','NEAR_METRO','METRO','ADJACENT']\n",
    "SCALE_COLS = ['PCT_LESS_HS', 'PCT_HS', 'PCT_SOME_BA', 'PCT_EQ_MORE_BA', 'PCT_POV_U18', \n",
    "              'PCT_POV_ALL', 'PCT_DEEP_POV_ALL', 'PCT_DEEP_POV_U18', 'PCT_CHG_EMPLOY_0710', 'PCT_CHG_EMPLOY_0718', \n",
    "              'PCT_CHG_EMPLOY_1018', 'PCT_CHG_EMPLOY_1718', 'AGRICULTURE', 'MINING', 'CONSTRUCTION',\n",
    "              'MANUFACTURING', 'TRADE', 'TRANSPORTATION', 'INFORMATION', 'FIRE', 'SERVICES', 'GOVERNMENT', 'POP_EMPLOYED']\n",
    "REPORT_PATH = PROJ + 'reports/'\n",
    "COLS_USE = DEMO_COLS + STD_COLS + SCALE_COLS\n",
    "enc = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalute the MCMC trace and compare to ml models\n",
    "def evaluate_trace(trace, X_train, X_test, y_train, y_test):\n",
    "    #fit the linear regression to the dataset and get the prediction\n",
    "    regressor = LinearRegression()  \n",
    "    regressor.fit(X_train[X_train.columns.difference(['REPUBLICAN_PCT'])], y_train)\n",
    "    y_pred = regressor.predict(X_test[X_test.columns.difference(['REPUBLICAN_PCT'])])\n",
    "    \n",
    "    # Dictionary of all sampled values for each parameter\n",
    "    var_dict = {}\n",
    "    for variable in trace.varnames:\n",
    "        var_dict[variable] = trace[variable]\n",
    "        \n",
    "    # Results into a dataframe\n",
    "    var_weights = pd.DataFrame(var_dict)\n",
    "    \n",
    "    # Means for all the weights\n",
    "    var_means = var_weights.mean(axis=0)\n",
    "    \n",
    "    # Create an intercept column\n",
    "    X_test['Intercept'] = 1\n",
    "    \n",
    "    # Align names of the test observations and means\n",
    "    names = X_test.columns[1:]\n",
    "    X_test = X_test.loc[:, names]\n",
    "    var_means = var_means[names]\n",
    "    \n",
    "    # Calculate estimate for each test observation using the average weights\n",
    "    results = pd.DataFrame(index = X_test.index, columns = ['estimate'])\n",
    "\n",
    "    for row in X_test.iterrows():\n",
    "        results.loc[row[0], 'estimate'] = np.dot(np.array(var_means), np.array(row[1]))\n",
    "\n",
    "    actual = np.array(y_test)\n",
    "    b_errors = results['estimate'] - actual\n",
    "    n_errors = y_pred - actual\n",
    "    \n",
    "    print('Bayesian LR RMSE: %0.2f' % np.sqrt(np.mean(b_errors ** 2)))\n",
    "    print('Normal LR RMSE: %0.2f' % np.sqrt(np.mean(n_errors ** 2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
