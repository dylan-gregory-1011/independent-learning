{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as db\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as T\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.chrisstucchio.com/blog/2017/bayesian_linear_regression.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/multilevel_modeling.html\n",
    "\n",
    "https://towardsdatascience.com/bayesian-linear-regression-in-python-using-machine-learning-to-predict-student-grades-part-2-b72059a8ac7e\n",
    "\n",
    "https://docs.pymc.io/notebooks/GLM-hierarchical.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/multilevel_modeling.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/getting_started\n",
    "\n",
    "https://github.com/WillKoehrsen/Data-Analysis/blob/master/bayesian_lr/Bayesian%20Linear%20Regression%20Project.ipynb\n",
    "\n",
    "https://docs.pymc.io/notebooks/GLM.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/bayesian_neural_network_advi.html\n",
    "\n",
    "http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf\n",
    "\n",
    "https://towardsdatascience.com/markov-chain-monte-carlo-in-python-44f7e609be98\n",
    "\n",
    "http://edwardlib.org\n",
    "\n",
    "https://stats.stackexchange.com/questions/252577/bayes-regression-how-is-it-done-in-comparison-to-standard-regression\n",
    "\n",
    "Bmik12#4%\n",
    "\n",
    "https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n",
    "\n",
    "https://docs.pymc.io/notebooks/rugby_analytics.html\n",
    "\n",
    "https://docs.pymc.io/notebooks/PyMC3_tips_and_heuristic.html\n",
    "\n",
    "https://dsaber.com/2016/08/27/analyze-your-experiment-with-a-multilevel-logistic-regression-using-pymc3%E2%80%8B/\n",
    "\n",
    "https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations\n",
    "\n",
    "https://docs.pymc.io/notebooks/GLM-logistic.html\n",
    "\n",
    "https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Corr = X.corr()['REPUBLICAN_PCT'].reset_index()\n",
    "X_Corr_list = X_Corr[(X_Corr['index'] != 'REPUBLICAN_PCT') \n",
    "                     & (abs(X_Corr['REPUBLICAN_PCT']) > 0.10)]['index'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGN_TRACE = {}\n",
    "\n",
    "for rgn in X['RGN_DESC'].unique():\n",
    "    X_RGN = X[X['RGN_DESC'] == rgn]\n",
    "    df12 = X_RGN[X_RGN['YR'] == 2012]\n",
    "    df12.drop(columns = ['RGN_DESC','YR'], inplace = True)\n",
    "    \n",
    "    df16 = X_RGN[X_RGN['YR'] == 2016]\n",
    "    df16.drop(columns = ['RGN_DESC','YR'], inplace = True)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df16, df16['REPUBLICAN_PCT'], test_size = 0.1, random_state=42)\n",
    "    \n",
    "    formula = \"REPUBLICAN_PCT ~ \" + \" + \".join(list(df12)[1:])\n",
    "    with pm.Model() as model:\n",
    "        #set variable means and sd\n",
    "        grp_mean = pm.Normal('grp_mean', mu= 50, sigma=10)\n",
    "        grp_sd = pm.Uniform('grp_sd', 0, 10)\n",
    "        \n",
    "        # The prior for the data likelihood is a Normal REPUBLICAN_PCT\n",
    "        #priors = {'Intercept': pm.Normal.dist(mu=df12['REPUBLICAN_PCT'].mean(), \n",
    "        #                                    sigma=df12['REPUBLICAN_PCT'].std())}\n",
    "        #for col in list(df12)[1:]:\n",
    "       #     priors[col] =  pm.Normal.dist(mu=grp_mean, sigma=grp_sd)\n",
    "\n",
    "        # Creating the model requires a formula and data (and optionally a family)\n",
    "        pm.GLM.from_formula(formula, data = X_train, family = pm.glm.families.Normal())# priors = priors,#)\n",
    "\n",
    "        # Perform Markov Chain Monte Carlo sampling letting PyMC3 choose the algorithm\n",
    "        trace = pm.sample(2000, step = pm.NUTS(), cores = 1)\n",
    "\n",
    "        model_formula = 'REPUBLICAN_PCT = '\n",
    "        for variable in trace.varnames:\n",
    "            model_formula += ' %0.2f * %s +' % (np.mean(trace[variable]), variable)\n",
    "        \n",
    "    RGN_TRACE[rgn] = trace\n",
    "    print(model_formula)\n",
    "    evaluate_trace(trace, X_train, X_test, y_train, y_test)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    # Priors\n",
    "    mu_a = pm.Normal('mu_a', mu=50., sigma=1e5)\n",
    "    sigma_a = pm.HalfCauchy('sigma_a', 5)\n",
    "    mu_b = pm.Normal('mu_b', mu=0., sigma=1e5)\n",
    "    sigma_b = pm.HalfCauchy('sigma_b', 5)\n",
    "\n",
    "    # Random intercepts\n",
    "    a = pm.Normal('intercept', mu= mu_a, sigma=sigma_a, shape = len(regions['i']))\n",
    "    \n",
    "    # Random slopes\n",
    "    beta = pm.Normal('beta', mu=mu_b, sigma=sigma_b, shape=len(regions['i']))\n",
    "    \n",
    "    #Regional random effects\n",
    "    #theta = pm.Normal('theta', mu=0.0, tau=sd_att, shape=N)\n",
    "    #mu_phi = CAR2('mu_phi', w=wmat2, a=amat2, tau=tau_c, shape=N)\n",
    "        \n",
    "    #lm_input = pm.Data('lm_input', X_train)\n",
    "    #beta = pm.Normal(\"beta\", mu=0, sigma=sd_att, shape=(len(list(X_train)), len(regions['i'])))\n",
    "    \n",
    "    #mu = pm.Deterministic('mu', tt.exp(tt.dot(X_train.values, beta[region]) + intercept[region]))\n",
    "        # Regional random effects\n",
    "    #theta = pm.Normal('theta', mu=0.0, tau=sd_att, shape=N)\n",
    "    #mu_phi = CAR2('mu_phi', w=wmat2, a=amat2, tau=tau_c, shape=N)\n",
    "    lm_input = pm.Data('lm_input', X_train)\n",
    "    beta = pm.Normal(\"mu_rgn\", mu=0, sigma=sd_att, shape=(len(list(X_train)), len(regions['i'])))\n",
    "    \n",
    "    mu = pm.Deterministic('mu', tt.exp(tt.dot(X_train.values, beta[region]) + a[region]))\n",
    "    \n",
    "    # Creating the model requires a formula and data (and optionally a family)\n",
    "    likelihood = pm.Normal('y', mu=mu.ravel(), sd=sigma, observed=y_train.values)\n",
    "\n",
    "    # Perform Markov Chain Monte Carlo sampling letting PyMC3 choose the algorithm\n",
    "    trace = pm.sample(2000, step = pm.NUTS(), cores = 1)\n",
    "\n",
    "    #model_formula = 'REPUBLICAN_PCT = '\n",
    "    #for variable in trace.varnames:\n",
    "    #    model_formula += ' %0.2f * %s +' % (np.mean(trace[variable]), variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nn(X, y):\n",
    "    n_hidden = 15\n",
    "\n",
    "    # Initialize random weights between each layer\n",
    "    init_1 = np.random.randn(X.shape[1], n_hidden).astype(floatX)\n",
    "    init_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "    init_out = np.random.randn(n_hidden).astype(floatX)\n",
    "\n",
    "    with pm.Model() as neural_network:\n",
    "        # Trick: Turn inputs and outputs into shared variables using the data container pm.Data\n",
    "        # It's still the same thing, but we can later change the values of the shared variable\n",
    "        # (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "        # Kind-of like a pointer we can redirect.\n",
    "        # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "        ann_input = pm.Data('ann_input', X)\n",
    "        ann_output = pm.Data('ann_output', y)\n",
    "\n",
    "        # Weights from input to hidden layer\n",
    "        weights_in_1 = pm.Normal('w_in_1', 0, sigma=1,\n",
    "                                 shape=(X.shape[1], n_hidden),\n",
    "                                 testval=init_1)\n",
    "\n",
    "        # Weights from 1st to 2nd layer\n",
    "        weights_1_2 = pm.Normal('w_1_2', 0, sigma=1,\n",
    "                                shape=(n_hidden, n_hidden),\n",
    "                                testval=init_2)\n",
    "\n",
    "        # Weights from hidden layer to output\n",
    "        weights_2_out = pm.Normal('w_2_out', 0, sigma=1,\n",
    "                                  shape=(n_hidden,),\n",
    "                                  testval=init_out)\n",
    "\n",
    "        # Build neural-network using tanh activation function\n",
    "        act_1 = pm.math.tanh(pm.math.dot(ann_input,\n",
    "                                         weights_in_1))\n",
    "        act_2 = pm.math.tanh(pm.math.dot(act_1,\n",
    "                                         weights_1_2))\n",
    "        act_out = pm.math.sigmoid(pm.math.dot(act_2,\n",
    "                                              weights_2_out))\n",
    "\n",
    "        # Binary classification -> Bernoulli likelihood\n",
    "        out = pm.Bernoulli('out',\n",
    "                           act_out,\n",
    "                           observed=ann_output,\n",
    "                           total_size=y.shape[0] # IMPORTANT for minibatches\n",
    "                          )\n",
    "    return neural_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = T.matrix('X')\n",
    "# symbolic number of samples is supported, we build vectorized posterior on the fly\n",
    "n = T.iscalar('n')\n",
    "# Do not forget test_values or set theano.config.compute_test_value = 'off'\n",
    "\n",
    "x.tag.test_value = np.empty_like(X_train[:10])\n",
    "n.tag.test_value = 100\n",
    "_sample_proba = approx.sample_node(neural_network.out.distribution.p,\n",
    "                                   size=n,\n",
    "                                   more_replacements={neural_network['ann_input']: x})\n",
    "# It is time to compile the function\n",
    "# No updates are needed for Approximation random generator\n",
    "# Efficient vectorized form of sampling is used\n",
    "sample_proba = theano.function([x, n], _sample_proba)\n",
    "\n",
    "pred = sample_proba(X_test, 500).mean(0) > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = construct_nn(X_train, y_train)\n",
    "\n",
    "with neural_network:\n",
    "    inference = pm.ADVI()\n",
    "    approx = pm.fit(n=30000, method=inference)\n",
    "\n",
    "    trace = approx.sample(draws=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(REPORT_PATH + 'bayesian_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(RGN_TRACE, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(REPORT_PATH + 'bayesian_model.pickle', 'rb') as handle:\n",
    "    RGN_TRACE = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
